***Пайплайн NLP***
==================

Багато задач взагалі вирішуються на рівні речення. Наприклад, машинний переклад. Найчастіше, ми просто переводимо одне речення і ніяк не використовуємо контекст ширшого рівня. Є завдання, де це не так, наприклад, діалогові системи. Тут важливо пам'ятати, про що систему запитували раніше, щоб вона могла відповісти на питання. Проте, речення - теж основна одиниця, з якої ми працюємо.

Тому перші два кроки пайплайна, які виконуються практично для вирішення будь-яких завдань - це сегментація (розподіл тексту на речення) і токенізація (розподіл пропозицій на токени, тобто окремі слова). Це робиться нескладними алгоритмами.

Далі потрібно обчислити ознаки кожного токена. Як правило, це відбувається в два етапи. Перший - обчислити контекстно-незалежні ознаки токена. Це набір ознак, які ніяк не залежать від оточуючих наш токен інших слів. Звичайні контекстно-незалежні ознаки - це:
* ембеддінги
* символьні ознаки
* додаткові ознаки, спеціальні для конкретного завдання або мови

Одна з найбільш використовуваних ознак - частина мови або POS-тег (part of speech). Такі ознаки можуть бути важливі для вирішення багатьох завдань, наприклад завдання синтаксичного парсинга. Для мов зі складною морфологією, типу української мови, також важливі морфологічні ознаки: наприклад, в якому відмінку стоїть іменник, який рід у прикметника. З цього можна зробити різні висновки про структуру речення. Також, морфологія потрібна для лематизації (приведення слів до початкових форм), за допомогою якої ми можемо скоротити розмірність простору ознак, і тому морфологічний аналіз активно використовується для більшості завдань NLP.

Коли ми вирішуємо завдання, де важлива взаємодія між різними об'єктами (наприклад, в задачі relation extraction або при створенні системи запитання-відповідь), нам потрібно багато чого знати про структуру пропозиції. Для цього потрібен синтаксичний розбір.

Ще одним прикладом додаткового ознаки є позиція токена в тексті. Ми можемо апріорі знати, що якась сутність частіше зустрічається на початку тексту або навпаки в кінці.

Всі разом - ембеддінги, символьні і додаткові ознаки - формують вектор ознак токена, який не залежить від контексту.

***Контекстно-залежні ознаки***
===============================

Контекстно-залежні ознаки токена - це набір ознак, який містить інформацію не тільки про сам токен, а й про його сусідів. Є різні способи обчислити ці ознаки. У класичних алгоритмах люди часто просто йшли «вікном»: брали кілька (наприклад, три) токени до вихідного і кілька токенів після, а потім вираховували всі ознаки в такому вікні. Такий підхід ненадійний, тому що важлива інформація для аналізу може перебувати на відстані, що перевищує вікно, відповідно, ми можемо щось пропустити.

Тому зараз всі контекстно-залежні ознаки обчислюються на рівні речення стандартним чином: за допомогою двосторонніх рекурентних нейромереж LSTM або GRU. Щоб отримати контекстно-залежні ознаки токена з контекстно-незалежних, контекстно-незалежні ознаки всіх токенов речення подаються в Bidirectional RNN (одно- або декілька- шаровий). Вихід Bidirectional RNN в i-ий момент часу і є контекстно-залежною ознакою i-того токена, який містить інформацію як про попередні токени (оскільки ця інформація міститься в i-му значенні прямого RNN), так і про подальші (окільки ця інформація міститься у відповідному значенні зворотного RNN).

Далі для кожної окремої задачі ми робимо щось своє, але перші кілька шарів - аж до Bidirectional RNN можна використовувати для практично будь-яких завдань.

Такий спосіб отримання ознак і зветься пайплайном NLP.

Варто відзначити, що в останні 2 роки дослідники активно намагаються вдосконалити пайплайн NLP - як з точки зору швидкодії (наприклад, transformer - архітектура, заснована на self-attention, не містить в собі RNN і тому здатна швидше навчатися і застосовуватися), так і з точки зору використовуваних ознак (зараз активно використовують ознаки на основі переднавчених мовних моделей, наприклад ELMo, або використовують перші шари переднавченої мовної моделі і вдосколналюють їх на наявному для завдання в корпусі - ULMFit, BERT).

***Словоформенні ембеддінги***
==============================

Давайте докладніше розберемо, що ж таке ембеддінг. Грубо кажучи, ембеддінг - це стисле уявлення про контекст слова. Чому важливо знати контекст слова? Тому що ми віримо в дистрибутивну гіпотезу - що схожі за змістом слова вживаються в подібних контекстах.

Давайте тепер спробуємо дати суворе визначення ембеддінга. Ембеддінг - це відображення з дискретного вектора категоріальних ознак в безперервний вектор із заздалегідь заданою розмірністю.

Канонічний приклад ембеддінга - це ембеддінг слова (словоформенний ембеддінг).

Що зазвичай виступає в ролі дискретного вектора ознак? Логічний вектор, відповідний всіляких значень якоїсь категорії (наприклад, всі можливі частини мови або всі можливі слова з якогось обмеженого словника).

Для словоформенних ембеддінгов такою категорією зазвичай виступає індекс слова в словнику. Припустимо, є словник розмірністю 100 тисяч. Відповідно, кожне слово має дискретний вектор ознак - логічний вектор розмірності 100 тисяч, де на одному місці (індексі даного слова в нашому словнику) знаходиться одиничка, а на інших - нулі.

Чому ми хочемо відображати наші дискретні вектори ознак в безперервні заданої розмірності? Тому що вектори розмірністю 100 тисяч не дуже зручно використовувати для обчислень, а ось вектори цілих чисел розмірності 100, 200 або, наприклад, 300, - набагато зручніше.

В принципі, ми можемо не намагатися накладати ніяких додаткових обмежень на таке відображення. Але якщо вже ми будуємо таке відображення, давайте будемо намагатись, щоб вектори схожих за змістом слів також були в якомусь сенсі близькі. Це робиться за допомогою простої feed-forward нейромережі.
