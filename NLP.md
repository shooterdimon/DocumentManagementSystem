# NLP
**Обробка природної мови (Natural Language Processing, NLP)** - загальний напрямок штучного інтелекту і математичної лінгвістики. Воно вивчає проблеми комп'ютерного аналізу і синтезу природних мов. Стосовно до штучного інтелекту, аналіз означає розуміння мови, а синтез - генерацію грамотного тексту. Вирішення цих проблем буде означати створення більш зручної форми взаємодії комп'ютера і людини. [1] До основних завдань обробки природної мови відносять такі, як:
* Формування відповідей на питання (Question Answering)
* Аналіз емоційного забарвлення висловлювань (Sentiment Analysis) 
* Знаходження тексту, відповідного зображенню (Image to Text Mappings) 
* Машинний переклад (Machine Translation)
* Розпізнавання мови (Speech Recognition)
* Морфологічна розмітка (Part of Speech Tagging)
* Витяг сутностей (Name Entity Recognition)

# **Способи аналізу**
### **Word2Vec**
В основі даної технології лежить уявлення слів у вигляді векторів заданої розмірності, маючи в своєму розпорядженні схожі слова близько один до одного. Тобто, відстань між векторами слів, що позначають схожі речі буде значно менше, ніж між словами, значення яких мають мало спільного. Дана особливість дозволяє більш гнучко представляти дані, які в подальшому можуть бути використані в навчанні нейронних мереж, різних класифікаторів. Для створення бази відповідностей "слово - вектор", алгоритм спочатку переглядає весь виданий йому текст, складаючи "словник", який в наступних ітераціях роботи алгоритму, буде використаний для визначення відповідних векторів. 

Існує два основні підходи: CBOW (Continuous Bag of Words) і Skip-gram. CBOW - «безперервний мішок зі словами" модельна архітектура, яка передбачає поточне слово, виходячи з навколишнього його контексту. Архітектура типу Skip-gram діє інакше: вона використовує поточний слово, щоб передбачати навколишні його слова. [3]
### **Визначення структури тексту**
Всі тексти на природній мові мають велику кількість слів, які не несуть інформації про даний текст. Наприклад, в англійській мові такими словами є артиклі. Дані слова називають шумовими або стоп-словами. Для досягнення кращої якості класифікації на першому етапі попередньої обробки текстів зазвичай необхідно видаляти такі слова. Другий етап попередньої обробки текстів - приведення кожного слова до основи, однаковою для всіх його граматичних форм. Це необхідно, тому що слова несуть один і той же сенс можуть бути записані в різній формі.
### **Нейронні мережі**
Штучні нейронні мережі являють собою систему з'єднаних і взаємодіючих між собою простих процесорів - штучних нейронів. Алгоритм роботи таких процесорів найчастіше вкрай простий.

**RNN / LSTM** - рекурентні нейронні мережі [4], які відрізняються від іншого типу мереж тим, що крім зв'язків, що переходять від одного нейрона до іншого безпосередньо, як в мережах прямого поширення, а також зв'язку, що проходять "в часі". Тобто, сигнал від одного нейрона на етапі t перейде до іншого (або цього ж) нейрона на етапі t + 1. Таким чином рекурентні нейронні мережі можуть зберігати інформацію в часі, тим самим "запам'ятовуючи" деякі дані. Дана їх особливість як раз дуже сильно допомагає в перекладі, класифікації та обробки природного тексту в цілому, так як наша мова влаштована таким чином, що деякі дані на початку блоку тексту, можуть вплинути на розуміння і / або переклад в його кінці. 

**CNN СНС** - надточні нейронні мережі найкраще показали себе в розпізнаванні об'єктів і образів на картинках, класифікації зображень, виділення особливостей і стисненні даних. Однак, їм знайшлося застосування і в обробці тексту.

**Seq2Seq** - універсальна бібліотека для Tensorflow [2], яка може використовуватися для машинного перекладу, визначення змісту тексту, моделювання діалогів, опису змісту зображень і т. д. Seq2Seq дозволяє створювати і навчати моделі нейронних мереж виду 'sequence to sequence'.

# **Пайплайн NLP**

Багато задач взагалі вирішуються на рівні речення. Наприклад, машинний переклад. Найчастіше, ми просто переводимо одне речення і ніяк не використовуємо контекст ширшого рівня. Є завдання, де це не так, наприклад, діалогові системи. Тут важливо пам'ятати, про що систему запитували раніше, щоб вона могла відповісти на питання. Проте, речення - теж основна одиниця, з якої ми працюємо.

Тому перші два кроки пайплайна, які виконуються практично для вирішення будь-яких завдань - це сегментація (розподіл тексту на речення) і токенізація (розподіл пропозицій на токени, тобто окремі слова). Це робиться нескладними алгоритмами.

Далі потрібно обчислити ознаки кожного токена. Як правило, це відбувається в два етапи. Перший - обчислити контекстно-незалежні ознаки токена. Це набір ознак, які ніяк не залежать від оточуючих наш токен інших слів. Звичайні контекстно-незалежні ознаки - це:
* ембеддінги
* символьні ознаки
* додаткові ознаки, спеціальні для конкретного завдання або мови

Одна з найбільш використовуваних ознак - частина мови або POS-тег (part of speech). Такі ознаки можуть бути важливі для вирішення багатьох завдань, наприклад завдання синтаксичного парсинга. Для мов зі складною морфологією, типу української мови, також важливі морфологічні ознаки: наприклад, в якому відмінку стоїть іменник, який рід у прикметника. З цього можна зробити різні висновки про структуру речення. Також, морфологія потрібна для лематизації (приведення слів до початкових форм), за допомогою якої ми можемо скоротити розмірність простору ознак, і тому морфологічний аналіз активно використовується для більшості завдань NLP.

Коли ми вирішуємо завдання, де важлива взаємодія між різними об'єктами (наприклад, в задачі relation extraction або при створенні системи запитання-відповідь), нам потрібно багато чого знати про структуру пропозиції. Для цього потрібен синтаксичний розбір.

Ще одним прикладом додаткового ознаки є позиція токена в тексті. Ми можемо апріорі знати, що якась сутність частіше зустрічається на початку тексту або навпаки в кінці.

Всі разом - ембеддінги, символьні і додаткові ознаки - формують вектор ознак токена, який не залежить від контексту.

# **Контекстно-залежні ознаки**

Контекстно-залежні ознаки токена - це набір ознак, який містить інформацію не тільки про сам токен, а й про його сусідів. Є різні способи обчислити ці ознаки. У класичних алгоритмах люди часто просто йшли «вікном»: брали кілька (наприклад, три) токени до вихідного і кілька токенів після, а потім вираховували всі ознаки в такому вікні. Такий підхід ненадійний, тому що важлива інформація для аналізу може перебувати на відстані, що перевищує вікно, відповідно, ми можемо щось пропустити.

Тому зараз всі контекстно-залежні ознаки обчислюються на рівні речення стандартним чином: за допомогою двосторонніх рекурентних нейромереж LSTM або GRU. Щоб отримати контекстно-залежні ознаки токена з контекстно-незалежних, контекстно-незалежні ознаки всіх токенов речення подаються в Bidirectional RNN (одно- або декілька- шаровий). Вихід Bidirectional RNN в i-ий момент часу і є контекстно-залежною ознакою i-того токена, який містить інформацію як про попередні токени (оскільки ця інформація міститься в i-му значенні прямого RNN), так і про подальші (окільки ця інформація міститься у відповідному значенні зворотного RNN).

Далі для кожної окремої задачі ми робимо щось своє, але перші кілька шарів - аж до Bidirectional RNN можна використовувати для практично будь-яких завдань.

Такий спосіб отримання ознак і зветься пайплайном NLP.

Варто відзначити, що в останні 2 роки дослідники активно намагаються вдосконалити пайплайн NLP - як з точки зору швидкодії (наприклад, transformer - архітектура, заснована на self-attention, не містить в собі RNN і тому здатна швидше навчатися і застосовуватися), так і з точки зору використовуваних ознак (зараз активно використовують ознаки на основі переднавчених мовних моделей, наприклад ELMo, або використовують перші шари переднавченої мовної моделі і вдосколналюють їх на наявному для завдання в корпусі - ULMFit, BERT).

# **Словоформенні ембеддінги**

Давайте докладніше розберемо, що ж таке ембеддінг. Грубо кажучи, ембеддінг - це стисле уявлення про контекст слова. Чому важливо знати контекст слова? Тому що ми віримо в дистрибутивну гіпотезу - що схожі за змістом слова вживаються в подібних контекстах.

Давайте тепер спробуємо дати суворе визначення ембеддінга. Ембеддінг - це відображення з дискретного вектора категоріальних ознак в безперервний вектор із заздалегідь заданою розмірністю.

Канонічний приклад ембеддінга - це ембеддінг слова (словоформенний ембеддінг).

Що зазвичай виступає в ролі дискретного вектора ознак? Логічний вектор, відповідний всіляких значень якоїсь категорії (наприклад, всі можливі частини мови або всі можливі слова з якогось обмеженого словника).

Для словоформенних ембеддінгов такою категорією зазвичай виступає індекс слова в словнику. Припустимо, є словник розмірністю 100 тисяч. Відповідно, кожне слово має дискретний вектор ознак - логічний вектор розмірності 100 тисяч, де на одному місці (індексі даного слова в нашому словнику) знаходиться одиничка, а на інших - нулі.

Чому ми хочемо відображати наші дискретні вектори ознак в безперервні заданої розмірності? Тому що вектори розмірністю 100 тисяч не дуже зручно використовувати для обчислень, а ось вектори цілих чисел розмірності 100, 200 або, наприклад, 300, - набагато зручніше.

В принципі, ми можемо не намагатися накладати ніяких додаткових обмежень на таке відображення. Але якщо вже ми будуємо таке відображення, давайте будемо намагатись, щоб вектори схожих за змістом слів також були в якомусь сенсі близькі. Це робиться за допомогою простої feed-forward нейромережі.

# **Grammarly: перевірка правопису і граматики**
У липні 2009 року два українця, Олексій Шевченко і Максим Литвин, заснували стартап Grammarly Inc. для створення однойменного онлайн-сервісу з перевірки граматики. Розроблений сервіс за допомогою алгоритмів штучного інтелекту автоматично виявляє потенційні граматичні, орфографічні, пунктуаційні, словесні і стильові помилки в текстах. Використовуючи технології штучного інтелекту, сервіс постійно поповнює базу рекомендацій за рахунок даних від користувачів. За словами розробників, Grammarly застосовує для аналізу виключно ті документи, які люди самі зберігають в базі сервісу. Система розділяє текст на речення і фрази, після чого обробляє їх за допомогою NLP і машинного навчання.

# **Samsung Group: Bixby**

29 березня 2017 року компанія Samsung представила віртуальний помічник зі штучним інтелектом - Bixby. Асистент від південнокорейського гіганта оснащений наступними функціями:
* Bixby Voice: ІІ-помічник дозволяє не торкаючись смартфона керувати його додатками.
* Bixby Vision: Активація додаткової реальності дозволяє за допомогою камери ідентифікувати різні предмети в live-режимі.
* Bixby reminder: Інтуїтивні нагадування дозволяють створювати пам'ятки про певний моменті життя.
* Bixby home: Найпопулярніші програми для користувача, розташовані в окремому вікні.
Також розробники стверджують, що для того, щоб Bixby ефективніше ідентифікував ваше голосове звернення, вам слід якомога частіше з ним спілкуватися.

# **Amazon Echo(Alexa), Google Home, Siri**
Вони використовують всі технології, про які ви чули.
Синтаксис: Вони роблять частину мовних тегів і розбору на більш ніж 60 мовах. У них є кілька тегеров і парсеров, деякі з яких є специфічними для додатка, і які роблять різні компроміси між швидкістю і якістю.
Семантика: розпізнавання сутностей в тексті, зіставлення цих сутностей з нашим графом знань, де це можливо, розмітка сутностей різними способами, аналіз ключових слів, щоб з'ясувати, які слова чи фрази належать до однієї і тієї ж речі, і так далі.
Вилучення знань: вивчення відносин між сутностями, розпізнавання подій, зіставлення сутностей між запитами і документами. З'ясування теми на сторінці і підведення підсумків зі сторінки. Аналіз настроїв, кластеризація по різним метрикам і т. Д.
Як це працює?

Вони побудовані на основі обробки природної мови (NLP) - процедури перетворення мови в слова, звуки і ідеї. Сервіси записують ваші слова. Дійсно, інтерпретація звуків вимагає великих обчислювальних ресурсів, запис вашої мови відправляється на сервери A для більш ефективного аналізу.

Технонологія розбиває ваші «замовлення» на окремі звуки. Потім він звертається до бази даних, що містить різні вимови слів, щоб визначити, які слова найбільш точно відповідають комбінації окремих звуків. Потім система визначає важливі слова, щоб зрозуміти завдання і виконати відповідні функції. Наприклад, якщо вона помітить такі слова, як «спорт» або «баскетбол», відкриється програма для занять спортом.

Сервери відправляють інформацію назад на ваш пристрій, і Alexa Siri Google Home може говорити. Якщо їм потрібно щось відповісти назад, вони будуть проходити через той же процес, який описаний вище, але в зворотному порядку.

# **IKEA: додаток для створення інтер'єру**

IKEA в березні 2018 року представили додаток на платформі ARCore з функцією доповненої реальності - IKEA Place. Воно призначене для віртуальної розміщення меблів. В IKEA Place зібрані цифрові моделі 3200 предметів різних категорій меблів: кухня, вітальня, офіс. Також в програму вбудована технологія візуального пошуку по каталогу: якщо навести камеру на цікавий для предмет, то система виведе на екран гаджета всю інформацію про нього. Додатково, функція дозволяє порівняти наявну меблі з тієї, яка пропонується в IKEA.
# **Amazon Comprehend**
Amazon Comprehend - це сервіс обробки природної мови (NLP), в якому для пошуку закономірностей і взаємозв'язків в тексті застосовуються технології машинного навчання.

Amazon Comprehend використовує машинне навчання для отримання аналітичних даних та взаємозв'язків з ваших неструктурованих даних. Сервіс визначає мову тексту, витягує ключові фрази, розпізнає людей, місця, бренди або події, визначає ступінь позитивності або негативності тексту, аналізує текст за допомогою токенізаціі і частин мови і в результаті автоматично групує набір текстових файлів за темами.

Amazon Comprehend - це повністю керований сервіс, тому вам не потрібно готувати сервери, створювати, навчати і розгортати моделі машинного навчання. Ви платите тільки за те, чим користуєтеся, без мінімальних платежів або авансових зобов'язань.
# **Google BERT**
BERT (Bidirectional Encoder Representations) допомагає ІІ-моделям отримати «загальне уявлення про мову» на великих корпусах нерозміченого тексту, наприклад з «Вікіпедії». Потім ці алгоритми можна використовувати для конкретних завдань.
# **Microsoft LUIS**
Розуміння мови (LUIS) Сервіс на основі машинного навчання для вбудовування природної мови в додатки, боти і пристрої IoT.


# **ЛІТЕРАТУРА**
1. Обробка природної мови - Режим доступу: https://ru.wikipedia.org/wiki/Обработка_естественного_языка
2. Neural Machine Translation (seq2seq) Tutorial - Режим доступу: https://www.tensorflow.org/tutorials/seq2seq
3. Word2vec - Режим доступу: https://ru.wikipedia.org/wiki/Word2vec
4. LSTM - мережі довгої короткостроковій пам'яті - Режим доступу: https://habrahabr.ru/company/wunderfund/blog/331310/
