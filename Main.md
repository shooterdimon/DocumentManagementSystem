# Документообіг

**Документоообіг** — це рух документів в установі від моменту створення або від одержання зі сторони до моменту передачі на зберігання до архіву.
Українське законодавство надає наступне визначення терміну: документообіг в установі — рух службових документів з моменту їх створення або одержання до завершення виконання або відправлення.

## Основні параметри

Об'єм документообігу складається з вхідних, вихідних та внутрішніх документів, які оброблені за період одного календарного року.
Основні етапи документообігу:
* прийом вхідної кореспонденції;
* обробка та реєстрація документів;
* контроль виконання документів;
* обробка та відправлення вихідної кореспонденції.

Загальна кількість документів кожного потоку за певний період часу (місяць, квартал, рік) становить обсяг.

## Електронний документообіг

**Електронний документообіг (обіг електронних документів)** — сукупність процесів створення, оброблення, правлення, передавання, одержання, зберігання, використання та знищення електронних документів, які виконуються із застосуванням перевірки цілісності та у разі необхідності з підтвердженням факту одержання таких документів.

## Основні принципи електронного документообігу

*  Однократна реєстрація документа
*  Паралельне виконання різних операцій з метою скорочення часу руху документів і підвищення оперативності їх виконання
*  Безперервність руху документа
*  Єдина база документарної інформації для централізованого зберігання документів і що виключає дублювання документів
*  Ефективно організована система пошуку документа
Відправлення та передавання електронних документів
Відправлення та передавання електронних документів здійснюються автором або посередником в електронній формі за допомогою засобів інформаційних, телекомунікаційних, інформаційно-телекомунікаційних систем або шляхом відправлення електронних носіїв, на яких записано цей документ.

Якщо автор і адресат у письмовій формі попередньо не домовилися про інше, датою і часом відправлення електронного документа вважаються дата і час, коли відправлення електронного документа не може бути скасовано особою, яка його відправила. У разі відправлення електронного документа шляхом пересилання його на електронному носії, на якому записано цей документ, датою і часом відправлення вважаються дата і час здавання його для пересилання.

Вимоги підтвердження факту одержання документа, встановлені законодавством у випадках відправлення документів рекомендованим листом або передавання їх під розписку, не поширюються на електронні документи. У таких випадках підтвердження факту одержання електронних документів здійснюється згідно з вимогами цього Закону.

Перевірка цілісності електронного документа проводиться шляхом перевірки електронного цифрового підпису.

## Зберігання електронних документів та архіви електронних документів

Суб'єкти електронного документообігу повинні зберігати електронні документи на електронних носіях інформації у формі, що дає змогу перевірити їх цілісність на цих носіях.

Строк зберігання електронних документів на електронних носіях інформації повинен бути не меншим від строку, встановленого законодавством для відповідних документів на папері.

У разі неможливості зберігання електронних документів на електронних носіях інформації протягом строку, встановленого законодавством для відповідних документів на папері, суб'єкти електронного документообігу повинні вживати заходів щодо дублювання документів на кількох електронних носіях інформації та здійснювати їх періодичне копіювання відповідно до порядку обліку та копіювання документів, встановленого законодавством. Якщо неможливо виконати зазначені вимоги, електронні документи повинні зберігатися у вигляді копії документа на папері (у разі відсутності оригіналу цього документа на папері). При копіюванні електронного документа з електронного носія інформації обов'язково здійснюється перевірка цілісності даних на цьому носії.

При зберіганні електронних документів обов'язкове додержання таких вимог: 
1. інформація, що міститься в електронних документах, повинна бути доступною для її подальшого використання; 
2. має бути забезпечена можливість відновлення електронного документа у тому форматі, в якому він був створений, відправлений або одержаний; 
3. у разі наявності повинна зберігатися інформація, яка дає змогу встановити походження та призначення електронного документа, а також дату і час його відправлення чи одержання.

Суб'єкти електронного документообігу можуть забезпечувати додержання вимог щодо збереження електронних документів шляхом використання послуг посередника, у тому числі архівної установи, якщо така установа додержується вимог цієї статті. Створення архівів електронних документів, подання електронних документів до архівних установ України та їх зберігання в цих установах здійснюється у порядку, визначеному законодавством.

# Natural Language Processing

**Обробка природної мови (Natural Language Processing, NLP)** - загальний напрямок штучного інтелекту і математичної лінгвістики. Воно вивчає проблеми комп'ютерного аналізу і синтезу природних мов. Стосовно до штучного інтелекту, аналіз означає розуміння мови, а синтез - генерацію грамотного тексту. Вирішення цих проблем буде означати створення більш зручної форми взаємодії комп'ютера і людини. [1]

До основних завдань обробки природної мови відносять такі, як [6]:
* Формування відповідей на питання (Question Answering)
* Аналіз емоційного забарвлення висловлювань (Sentiment Analysis) 
* Знаходження тексту, відповідного зображенню (Image to Text Mappings) 
* Машинний переклад (Machine Translation)
* Розпізнавання мови (Speech Recognition)
* Морфологічна розмітка (Part of Speech Tagging)
* Витяг сутностей (Name Entity Recognition)

## Способи аналізу

### Word2Vec

В основі даної технології лежить уявлення слів у вигляді векторів заданої розмірності, маючи в своєму розпорядженні схожі слова близько один до одного. Тобто, відстань між векторами слів, що позначають схожі речі буде значно менше, ніж між словами, значення яких мають мало спільного. Дана особливість дозволяє більш гнучко представляти дані, які в подальшому можуть бути використані в навчанні нейронних мереж, різних класифікаторів. Для створення бази відповідностей "слово - вектор", алгоритм спочатку переглядає весь виданий йому текст, складаючи "словник", який в наступних ітераціях роботи алгоритму, буде використаний для визначення відповідних векторів. 

Існує два основні підходи: **CBOW** (Continuous Bag of Words) і **Skip-gram**. CBOW - «безперервний мішок зі словами" модельна архітектура, яка передбачає поточне слово, виходячи з навколишнього його контексту. Архітектура типу Skip-gram діє інакше: вона використовує поточний слово, щоб передбачати навколишні його слова. [3]

### Визначення структури тексту

Всі тексти на природній мові мають велику кількість слів, які не несуть інформації про даний текст. Наприклад, в англійській мові такими словами є артиклі. Дані слова називають шумовими або стоп-словами. Для досягнення кращої якості класифікації на першому етапі попередньої обробки текстів зазвичай необхідно видаляти такі слова. Другий етап попередньої обробки текстів - приведення кожного слова до основи, однаковою для всіх його граматичних форм. Це необхідно, тому що слова несуть один і той же сенс можуть бути записані в різній формі.

### Нейронні мережі

Штучні нейронні мережі являють собою систему з'єднаних і взаємодіючих між собою простих процесорів - штучних нейронів. Алгоритм роботи таких процесорів найчастіше вкрай простий.

**RNN / LSTM** - рекурентні нейронні мережі [4], які відрізняються від іншого типу мереж тим, що крім зв'язків, що переходять від одного нейрона до іншого безпосередньо, як в мережах прямого поширення, а також зв'язку, що проходять "в часі". Тобто, сигнал від одного нейрона на етапі t перейде до іншого (або цього ж) нейрона на етапі t + 1. Таким чином рекурентні нейронні мережі можуть зберігати інформацію в часі, тим самим "запам'ятовуючи" деякі дані. Дана їх особливість як раз дуже сильно допомагає в перекладі, класифікації та обробки природного тексту в цілому, так як наша мова влаштована таким чином, що деякі дані на початку блоку тексту, можуть вплинути на розуміння і / або переклад в його кінці. 

**CNN СНС** - надточні нейронні мережі найкраще показали себе в розпізнаванні об'єктів і образів на картинках, класифікації зображень, виділення особливостей і стисненні даних. Однак, їм знайшлося застосування і в обробці тексту.

**Seq2Seq** - універсальна бібліотека для Tensorflow [2], яка може використовуватися для машинного перекладу, визначення змісту тексту, моделювання діалогів, опису змісту зображень і т. д. Seq2Seq дозволяє створювати і навчати моделі нейронних мереж виду 'sequence to sequence'.

## Пайплайн NLP

Багато задач взагалі вирішуються на рівні речення. Наприклад, машинний переклад. Найчастіше, ми просто переводимо одне речення і ніяк не використовуємо контекст ширшого рівня. Є завдання, де це не так, наприклад, діалогові системи. Тут важливо пам'ятати, про що систему запитували раніше, щоб вона могла відповісти на питання. Проте, речення - теж основна одиниця, з якої ми працюємо.

Тому перші два кроки пайплайна, які виконуються практично для вирішення будь-яких завдань - це сегментація (розподіл тексту на речення) і токенізація (розподіл пропозицій на токени, тобто окремі слова). Це робиться нескладними алгоритмами.

Далі потрібно обчислити ознаки кожного токена. Як правило, це відбувається в два етапи. Перший - обчислити контекстно-незалежні ознаки токена. Це набір ознак, які ніяк не залежать від оточуючих наш токен інших слів. **Звичайні контекстно-незалежні ознаки** - це:
* ембеддінги
* символьні ознаки
* додаткові ознаки, спеціальні для конкретного завдання або мови

Одна з найбільш використовуваних ознак - частина мови або POS-тег (part of speech). Такі ознаки можуть бути важливі для вирішення багатьох завдань, наприклад завдання синтаксичного парсинга. Для мов зі складною морфологією, типу української мови, також важливі морфологічні ознаки: наприклад, в якому відмінку стоїть іменник, який рід у прикметника. З цього можна зробити різні висновки про структуру речення. Також, морфологія потрібна для лематизації (приведення слів до початкових форм), за допомогою якої ми можемо скоротити розмірність простору ознак, і тому морфологічний аналіз активно використовується для більшості завдань NLP.

Коли ми вирішуємо завдання, де важлива взаємодія між різними об'єктами (наприклад, в задачі relation extraction або при створенні системи запитання-відповідь), нам потрібно багато чого знати про структуру пропозиції. Для цього потрібен синтаксичний розбір.

Ще одним прикладом додаткового ознаки є позиція токена в тексті. Ми можемо апріорі знати, що якась сутність частіше зустрічається на початку тексту або навпаки в кінці.

Всі разом - ембеддінги, символьні і додаткові ознаки - формують вектор ознак токена, який не залежить від контексту.

## Контекстно-залежні ознаки

**Контекстно-залежні ознаки токена** - це набір ознак, який містить інформацію не тільки про сам токен, а й про його сусідів. Є різні способи обчислити ці ознаки. У класичних алгоритмах люди часто просто йшли «вікном»: брали кілька (наприклад, три) токени до вихідного і кілька токенів після, а потім вираховували всі ознаки в такому вікні. Такий підхід ненадійний, тому що важлива інформація для аналізу може перебувати на відстані, що перевищує вікно, відповідно, ми можемо щось пропустити.

Тому зараз всі контекстно-залежні ознаки обчислюються на рівні речення стандартним чином: за допомогою двосторонніх рекурентних нейромереж LSTM або GRU. Щоб отримати контекстно-залежні ознаки токена з контекстно-незалежних, контекстно-незалежні ознаки всіх токенов речення подаються в Bidirectional RNN (одно- або декілька- шаровий). Вихід Bidirectional RNN в i-ий момент часу і є контекстно-залежною ознакою i-того токена, який містить інформацію як про попередні токени (оскільки ця інформація міститься в i-му значенні прямого RNN), так і про подальші (окільки ця інформація міститься у відповідному значенні зворотного RNN).

Далі для кожної окремої задачі ми робимо щось своє, але перші кілька шарів - аж до Bidirectional RNN можна використовувати для практично будь-яких завдань.

Такий спосіб отримання ознак і зветься пайплайном NLP.

Варто відзначити, що в останні 2 роки дослідники активно намагаються вдосконалити пайплайн NLP - як з точки зору швидкодії (наприклад, transformer - архітектура, заснована на self-attention, не містить в собі RNN і тому здатна швидше навчатися і застосовуватися), так і з точки зору використовуваних ознак (зараз активно використовують ознаки на основі переднавчених мовних моделей, наприклад ELMo, або використовують перші шари переднавченої мовної моделі і вдосколналюють їх на наявному для завдання в корпусі - ULMFit, BERT).

## Словоформенні ембеддінги

Давайте докладніше розберемо, що ж таке ембеддінг. Грубо кажучи, ембеддінг - це стисле уявлення про контекст слова. Чому важливо знати контекст слова? Тому що ми віримо в дистрибутивну гіпотезу - що схожі за змістом слова вживаються в подібних контекстах.

Давайте тепер спробуємо дати суворе визначення ембеддінга. **Ембеддінг** - це відображення з дискретного вектора категоріальних ознак в безперервний вектор із заздалегідь заданою розмірністю.

**Канонічний приклад ембеддінга** - це ембеддінг слова (словоформенний ембеддінг).

Що зазвичай виступає в ролі дискретного вектора ознак? Логічний вектор, відповідний всіляких значень якоїсь категорії (наприклад, всі можливі частини мови або всі можливі слова з якогось обмеженого словника).

Для словоформенних ембеддінгов такою категорією зазвичай виступає індекс слова в словнику. Припустимо, є словник розмірністю 100 тисяч. Відповідно, кожне слово має дискретний вектор ознак - логічний вектор розмірності 100 тисяч, де на одному місці (індексі даного слова в нашому словнику) знаходиться одиничка, а на інших - нулі.

Чому ми хочемо відображати наші дискретні вектори ознак в безперервні заданої розмірності? Тому що вектори розмірністю 100 тисяч не дуже зручно використовувати для обчислень, а ось вектори цілих чисел розмірності 100, 200 або, наприклад, 300, - набагато зручніше.

В принципі, ми можемо не намагатися накладати ніяких додаткових обмежень на таке відображення. Але якщо вже ми будуємо таке відображення, давайте будемо намагатись, щоб вектори схожих за змістом слів також були в якомусь сенсі близькі. Це робиться за допомогою простої feed-forward нейромережі.

# Сфери використання NLP
## Grammarly: перевірка правопису і граматики

У липні 2009 року два українця, Олексій Шевченко і Максим Литвин, заснували стартап Grammarly Inc. для створення однойменного онлайн-сервісу з перевірки граматики. Розроблений сервіс за допомогою алгоритмів штучного інтелекту автоматично виявляє потенційні граматичні, орфографічні, пунктуаційні, словесні і стильові помилки в текстах. Використовуючи технології штучного інтелекту, сервіс постійно поповнює базу рекомендацій за рахунок даних від користувачів. За словами розробників, Grammarly застосовує для аналізу виключно ті документи, які люди самі зберігають в базі сервісу. Система розділяє текст на речення і фрази, після чого обробляє їх за допомогою NLP і машинного навчання.

## Samsung Group: Bixby

29 березня 2017 року компанія Samsung представила віртуальний помічник зі штучним інтелектом - Bixby. Асистент від південнокорейського гіганта оснащений наступними функціями:
* Bixby Voice: ІІ-помічник дозволяє не торкаючись смартфона керувати його додатками.
* Bixby Vision: Активація додаткової реальності дозволяє за допомогою камери ідентифікувати різні предмети в live-режимі.
* Bixby reminder: Інтуїтивні нагадування дозволяють створювати пам'ятки про певний моменті життя.
* Bixby home: Найпопулярніші програми для користувача, розташовані в окремому вікні.
Також розробники стверджують, що для того, щоб Bixby ефективніше ідентифікував ваше голосове звернення, вам слід якомога частіше з ним спілкуватися.

## Amazon Echo(Alexa), Google Home, Siri
Вони використовують всі технології, про які ви чули.
Синтаксис: Вони роблять частину мовних тегів і розбору на більш ніж 60 мовах. У них є кілька тегеров і парсеров, деякі з яких є специфічними для додатка, і які роблять різні компроміси між швидкістю і якістю.
Семантика: розпізнавання сутностей в тексті, зіставлення цих сутностей з нашим графом знань, де це можливо, розмітка сутностей різними способами, аналіз ключових слів, щоб з'ясувати, які слова чи фрази належать до однієї і тієї ж речі, і так далі.
Вилучення знань: вивчення відносин між сутностями, розпізнавання подій, зіставлення сутностей між запитами і документами. З'ясування теми на сторінці і підведення підсумків зі сторінки. Аналіз настроїв, кластеризація по різним метрикам і т. Д.
Як це працює?

Вони побудовані на основі обробки природної мови (NLP) - процедури перетворення мови в слова, звуки і ідеї. Сервіси записують ваші слова. Дійсно, інтерпретація звуків вимагає великих обчислювальних ресурсів, запис вашої мови відправляється на сервери A для більш ефективного аналізу.

Технонологія розбиває ваші «замовлення» на окремі звуки. Потім він звертається до бази даних, що містить різні вимови слів, щоб визначити, які слова найбільш точно відповідають комбінації окремих звуків. Потім система визначає важливі слова, щоб зрозуміти завдання і виконати відповідні функції. Наприклад, якщо вона помітить такі слова, як «спорт» або «баскетбол», відкриється програма для занять спортом.

Сервери відправляють інформацію назад на ваш пристрій, і Alexa Siri Google Home може говорити. Якщо їм потрібно щось відповісти назад, вони будуть проходити через той же процес, який описаний вище, але в зворотному порядку.

## IKEA: додаток для створення інтер'єру

**IKEA** в березні 2018 року представили додаток на платформі ARCore з функцією доповненої реальності - IKEA Place. Воно призначене для віртуальної розміщення меблів. В IKEA Place зібрані цифрові моделі 3200 предметів різних категорій меблів: кухня, вітальня, офіс. Також в програму вбудована технологія візуального пошуку по каталогу: якщо навести камеру на цікавий для предмет, то система виведе на екран гаджета всю інформацію про нього. Додатково, функція дозволяє порівняти наявну меблі з тієї, яка пропонується в IKEA.

## Amazon Comprehend
**Amazon Comprehend** - це сервіс обробки природної мови (NLP), в якому для пошуку закономірностей і взаємозв'язків в тексті застосовуються технології машинного навчання.

Amazon Comprehend використовує машинне навчання для отримання аналітичних даних та взаємозв'язків з ваших неструктурованих даних. Сервіс визначає мову тексту, витягує ключові фрази, розпізнає людей, місця, бренди або події, визначає ступінь позитивності або негативності тексту, аналізує текст за допомогою токенізаціі і частин мови і в результаті автоматично групує набір текстових файлів за темами.

Amazon Comprehend - це повністю керований сервіс, тому вам не потрібно готувати сервери, створювати, навчати і розгортати моделі машинного навчання. Ви платите тільки за те, чим користуєтеся, без мінімальних платежів або авансових зобов'язань.

## Google BERT
**BERT** (Bidirectional Encoder Representations) допомагає ІІ-моделям отримати «загальне уявлення про мову» на великих корпусах нерозміченого тексту, наприклад з «Вікіпедії». Потім ці алгоритми можна використовувати для конкретних завдань.

## Microsoft LUIS
Розуміння мови (LUIS) Сервіс на основі машинного навчання для вбудовування природної мови в додатки, боти і пристрої IoT.

# Синтаксичний аналіз і його методи

З виникненням Інформаційної ери і прийняттям її громадськістю  на початку 90-х років, перед людством постала проблема у конкретному аналізі цієї інформації та її структуруванні. З цією метою використовуються різні методики синтаксичного розбору (англ. parsing) в лінгвістиці та інформатиці.
Синтаксичний аналіз – це процес аналізу вхідної послідовності символів і розбір її граматичної структури згідно з заданою формальною граматикою за допомогою спеціального програмного забезпечення. 
Під час синтаксичного аналізу текст оформлюється у структуру даних, зазвичай – в дерево, яка підлягає подальшій обробці при пошуку актуальних і поточних в ній даних. Зазвичай аналізатори працюють в два етапи: 
1. Ідентифікуються осмислені токени(об’єкт, що утворюється в процесі лексичного аналізу)
2. Створюється дерево розбору(синтаксичне дерево), абстрактне дерево синтаксису або інша ієрархічна структура.

Синтаксичний аналіз(парсинг) є важливою складовою опрацювання тексту і спрямований на розпізнавання, виділення та групування даних. Загалом є найефективнішим рішенням для автоматизації збору та зміни інформації у її великих обсягах.
Парсер повинен мати такі характеристики:
* забезпечення швидкого обігу великої кількості інформації 
* грамотне відділення технічних даних від іншої інформації
* безпомилкове виявлення потрібної інформації 
* ефективна реалізація передачі та збереження даних

На етапі вилучення/перетворення даних використовується простий синтаксичний аналіз, який виконується завдяки регулярним виразам і дозволяє узгодити шаблон та вилучення тексту. Приклади: 
* функція читання файлів(HTML, XML-текст, зокрема мови розмітки даних) у программі
* у випадку мов програмування, аналізатор є компонентом компілятора або інтепретатора

## Схема роботи синтаксичного аналізатора

Інакше кажучи, аналізатор визначає в який спосіб вхідні дані можуть бути отримані з початкового символу граматики. Два основних методи: 
Нисхідний парсинг – знаходить лівіший диференційований елемент вхідного потоку за рахунок пошуку дерев розбору, використовуючи розширення даних формальних правил граматики згори донизу. Виключний вибір використовується для узгодження багатозначностей шляхом розширення всіх альтернативних правобічних правил. Реалізовується методом рекурсивного спуску.
Висхідний аналізатор намагається знайти основні елементи, потім елементи, що містять їх і тд. 

# Підсистеми пошуку (Повнотекстовий пошук)

**Повнотекстовий пошук (англ. Full text searching, фр. Recherche en texte integral)** - автоматизований пошук документів, при якому пошук ведеться не по іменах документів, а по їх вмісту, всьому або істотної частини.
Загалом, існує компроміс між "точністю" і "відкликанням". Висока точність означає, що представлено менше нерелевантних результатів (немає помилкових спрацьовувань), в той час як високий відгук означає, що менше релевантних результатів відсутній (немає помилкових негативів). Використання оператора LIKE дає 100% точність без поступок для відкликання. Повнотекстовий пошук дає велику гнучкість для налаштування точності для кращого відкликання.

У більшості повнотекстових пошукових реалізацій використовується "інвертований індекс". Це індекс, де ключі є окремими термінами, а пов'язані значення - це групи записів, які містять цей термін. Повнотекстовий пошук оптимізований для обчислення перетинів, об'єднань і т.д. Цих наборів записів і зазвичай надає алгоритм ранжирування для кількісної оцінки того, наскільки сильно дана запис відповідає ключовим словами пошуку.
Оператор SQL LIKE може бути вкрай неефективним. Якщо застосувати його до колонку без індексації, для пошуку збігів буде використовуватися повне сканування (точно так само як будь-який запит в неіндексованих поле). Якщо індекс індексується, зіставлення може виконуватися за допомогою індексних ключів, але з набагато меншою ефективністю, ніж більшість запитів індексу. У гіршому випадку шаблон LIKE матиме основні шаблони, які вимагають, щоб кожен індексний ключ був перевірений. Навпаки, багато інформаційно-пошукові системи можуть підтримувати підтримку провідних символів узагальнення шляхом попередньої компіляції суфіксів в обраних полях.

## Інші функції, характерні для повнотекстового пошуку:

1. лексичний аналіз або токенізація; блок неструктурованого тексту в окремі слова, фрази і спеціальні маркери
2. морфологічний аналіз, або згортання змін даного слова в один індексний термін; наприклад, лікування "мишей" і "миша" або "електрифікація" і "електричний" як одне і те ж слово
3. рейтинг - вимір схожість збігається записи з рядок запиту
повнотекстовий індекс

Перші версії програм повнотекстового пошуку припускали сканування всього вмісту всіх документів в пошуку заданого слова або фрази. При використанні такої технології пошук займав дуже багато часу (в залежності від розміру бази), а в інтернеті був би нездійсненний. Сучасні алгоритми заздалегідь формують для пошуку так званий повнотекстовий індекс - словник, в якому перераховані всі слова і вказано, в яких місцях вони зустрічаються. При наявності такого індексу досить здійснити пошук потрібних слів в ньому і тоді відразу ж буде отримано список документів, в яких вони зустрічаються.

У повнотекстовий індекс включається один або кілька символьних стовпців таблиці. Ці стовпці можуть містити будь-який з наступних типів даних: char, varchar, nchar, nvarchar, text, ntext, image, xml або varbinary (max) і FILESTREAM. Кожний повнотекстовий індекс індексує один або декілька стовпців таблиці, а кожній колонці може відповідати певна мова.

## Порівняння предиката LIKE і запитів повнотекстового пошуку

На відміну від повнотекстового пошуку предикат LIKE Transact-SQL працює тільки з комбінаціями символів. Крім того, предикат LIKE можна використовувати в запитах до форматованим двійковим даними. Більш того, запит з предикатом LIKE до великої кількості неструктурованих текстових даних виконується набагато повільніше, ніж еквівалентний повнотекстовий запит до тих самих даних. Виконання запиту LIKE до кільком мільйонам рядків текстових даних може зайняти кілька хвилин, в той час як повнотекстовий запит до тих же даних займає всього кілька секунд або навіть менше, залежно від кількості повертаються рядків.

## Обробка повнотекстових запитів

Оброблювач запитів передає для обробки повнотекстові частини запиту засобу повнотекстового пошуку. Засіб повнотекстового пошуку виконує розбиття за словами і при необхідності розширення тезауруса, морфологічний пошук і обробку стоп-слів (пропускаються слів). Потім повнотекстові частини запиту подаються у формі операторів SQL, в основному як потокові функції, які повертають табличні значення. Під час виконання запиту ці потокові функції для отримання правильних результатів звертаються до інвертований індекс. Результати повертаються клієнтові в цій точці, або перед поверненням клієнтові вони піддаються додатковій обробці.

## Структура повнотекстового індексу

Щоб зрозуміти, як працює засіб повнотекстового пошуку, необхідно розібратися в структурі повнотекстового індексу. Багато СУБД підтримують методи повнотекстового пошуку (Fulltext search), які дозволяють дуже швидко знаходити потрібну інформацію у великих обсягах тексту. На відміну від оператора LIKE, такий тип пошуку передбачає створення відповідного повнотекстового індексу, який представляє собою своєрідний словник згадок слів в полях. Під словом розуміється сукупність з не менш 3-х не пробільних символів (але це може бути змінено). Залежно від даних словника може бути обчислена релевантність – порівняльна міра відповідності запиту знайденої інформації. У статті розповідається як працювати з повнотекстовим пошуком на прикладі БД MySQL, а так само наведу приклади «нестандартного» використання даного механізму. В MySQL можливості повнотекстового пошуку (лише для MyISAM-таблиць) підтримуються починаючи з версії 3.23.23. В наступних версіях механізм зазнав істотні доопрацювання і розширення, в тозі перетворившись на потужний засіб для створення пошукових механізмів веб-додатків. Головна особливість – швидкий пошук слів в дуже великих обсягах текстової інформації. 

## Індекс FULLTEXT

Отже, щоб працювати з повнотекстовим пошуком, спочатку нам потрібно створити відповідний індекс. Він називається FULLTEXT, і може бути накладено на поля CHAR, VARCHAR і TEXT. Причому, як і у випадку зі звичайним індексом – якщо відбувається пошук по 2-м полям, то потрібен об'єднаний індекс 2-х полів, використовуйте пошук по одному полю – потрібен індекс тільки цього поля. 

У цьому прикладі створюється таблиця з 2-ма повнотекстовими індексами: ft1 і ft2, які можна використовувати для пошуку в полях title і body, або тільки в body.  Тільки в поле title шукати не вийде.

## Конструкція MATCH-AGAINST

Власне для самого повнотекстового пошуку в MySQL використовується конструкція MATCH (filelds) ... AGAINST (words).  Вона може працювати в різних режимах, які досить сильно між собою відрізняються.  Для всіх діє наступне правило: дана конструкція повертає умовну релевантність, але спосіб обчислення якої може бути різним у залежності від режиму.  Ще варто додати що у всіх режимах пошук завжди чутливі до регістру.

# Великий електронний словник української мови (ВЕСУМ)

Основою більшості засобів NLP є словник тегів частин мови (англ. POS tag dictionary). Такий словник містить дані про відмінювання слів та має відповідні теги для кожної форми слова. Скажімо, слово різав матиме базову форму (лему) різатий теги дієслова минулого часу, однини, чоловічого роду.
Така інформація дає змогу шукати слово за будь-якою його формою, а також проводити аналіз речень: перевіряти, чи узгоджені між собою слова в реченні, фільтрувати слова за родом, відмінком тощо.

**Словник**:

* налічує понад 285 тис. лем і постійно поповнюється
* містить інформацію про відмінювання слів
* подає нерекомендовані слова (активні дієприкметники, невдалі кальки тощо) та заміну для них
* охоплює абревіатури та скорочення
* містить інформацію про деякі альтернативні правописні варіанти (дає змогу аналізувати тексти, написані не за чинним правописом, адже низка медій, авторів, видавництв свідомо дотримуються альтернативних правописних правил)
* має велику базу власних імен (зокрема українських імен, по батькові та прізвищ, неукраїнських імен та прізвищ, українських та закордонних топонімів тощо)
* синхронізований з КОАТУУ, зокрема містить назви, що з’явилися внаслідок декомунізації
* має дуже компактну систему позначення відмінювання та тегів для слів, завдяки чому легко додавати нові слова, групувати наявні тощо
* містить інформацію про деякі рідкісні та розмовні форми, наприклад, нестягнені форми прикметників (гарная), та розмовні інфінітиви (поїхать); щоправда, більшість таких форм вимкнено за уставою, оскільки вони мають обмежену форму застосування й часто створюють зайву омонімію (однак за потреби їх можна ввімкнути)
* є відкритим проектом (розміщений на github), тож кожен може долучитися до вдосконалення та використовувати його у своїй роботі.

## Структура словника

**Словник містить три основні частини**:
1. правила зміни суфіксів у парадигмах
2. слова з прапорцями парадигм та додаткових властивостей
3. код генерування повних парадигм із сирцевої інформації (1 та 2)

## Орфографічні словники

Орфографічні словники є найпростішим застосуванням. Вони використовують лише наявність словоформи у словнику. ВЕСУМ є сирцем для українських словників перевірки орфографії системи hunspell, який широко використовується у відкритій ОС Linux, а також у багатьох інших програмних засобах, зокрема у словниках для браузера Firefox, офісних пакетів LibreOffice.org/OpenOffice.

## Повнотекстовий пошук

Одним із найпопулярніших відкритих систем повнотекстового пошуку є **Lucene**.
Більшість модулів аналізу в Lucene для європейських мов використовують стемери. Стемери працюють шляхом відкидання закінчень слів і зіставлення лише центральної, нефлексійної частини (основи). Це непогано працює для мов, що мають не надто розвинуту флексійну систему, але є значно гіршим розв’язком для східнослов’янських мов, що мають велику кількість відмінкових форм. Ситуацію з українською мовою ускладнює й наявність численних чергувань в корені слів та варіантних форм.
Український аналізатор в Lucene використовує лематизатор зі словником у форматі morfologik, що базується на скінченних автоматах (finite-state automata). Цей засіб є надзвичайно ефективним як із погляду швидкості пошуку, так і з погляду використання пам’яті — якщо нормалізована за лемою текстова версія українського словника займає 185 МБ, то у форматі morfologik — лише 1,8 МБ.
Позаяк ми ще раніше мали український словник для morfologik, який використовували в LanguageTool, лишалося тільки додати український модуль аналізу, прив’язати його до словника й доповнити його деякими допоміжними деталями: ігнорованими словами (stopwords), різними формами апострофів, ігноруванням наголосу тощо.
Український аналізатор побачив світ у версії Lucene 6.2.0 (і як наслідок в ElasticSearch та інших двигунах, що використовують Lucene). Нещодавноу фонді Wikimedia закінчили тестуватий ухвалили використовувати цей український модуль для пошуку в українській Вікіпедії (до цього для української користувалися російським повнотекстовим пошуком, з відповідними результатами).
Одним із недоліків пошуку за словниковим лематизатором є те, що лематизація не працює, якщо слово відсутнє в словнику. Це може значно знизити ефективність пошуку для деяких типів текстів. Скажімо, українська Вікіпедія має надвисоку частку власних назв у статтях. Додати навіть більшість з них у словник неможливо, тож потрібно шукати інший розв’язок. Один зі шляхів усунення цієї проблеми — додати модуль динамічної стемізації/лематизації для невідомих слів. Проект ВЕСУМ охоплює майже всі варіанти логіки відмінювання українських слів (понад 4000), тож можна було б побудувати спрощену логіку, що генерує потенційні леми.

## Лематизація

Окрім повнотекстового пошуку лематизація також використовується для побудови деяких моделей NLP, наприклад, word embeddings. Так, свого часу ми використовували лематизовану версію великого корпусу українських текстів для побудови векторів слів (word vectors) для проекту lang-uk. Через те, що українська мова має складну морфологію та велику кількість словоформ, було доволі складно побудувати ефективну модель, котра б ефективно узагальнювала ці словоформи та надавала б їм схожі вектори. До того ж, через величезну кількість словоформ словник корпусу виходив доволі великим, й дуже багато словоформ було зафіксовано лише один чи два рази. Використання лематизації допомогло скоротити словник та побудувати на лематизованому тексті корпусу якісну модель, що показала гарні результати протягом внутрішнього тестування.

## Морфологічний аналіз

Оскільки ВЕСУМ містить теги частин мови, то маючи таку інформацію, можна будувати досить складні правила аналізу граматичних зв’язків між словами в реченні. Одним з перших програмних засобів, що працюють з такими тегами, є український модуль програмного засобу перевірки граматики та стилю LanguageTool.
Інше застосування морфологічного аналізу — побудова текстового корпусу.

## Інші застосування:

* укладання тлумачних, термінологічних, перекладних та інших типів словників (зокрема пошук прикладів вживання)
* різноманітні мовознавчі дослідження
* дослідження і розробки у галузі комп’ютерної лінгвістики (зокрема побудова моделей мови, отримання статистичної інформації)
* довідкові функції та редагування

## Граматичний аналіз

Щоб якісно аналізувати текст, окрім словника тегів потрібно мати програмні засоби, що спочатку готують текст для тегування, потім його тегують, і проводять післяоброблювання.
Хоча ВЕСУМ є відкритим, загальним словником і має потенційно необмежену сферу застосування, він народився з потреб українського модуля LanguageTool, тож у процесі аналізу текстів української мови він найкраще працює вкупі з аналізатором LanguageTool.
Розвиток ВЕСУМ-ай українського аналізатора в LT вже декілька років ідуть разом в ітераційній послідовності. Зміни в словнику проходять тести LT, що дає змогу побачити, як кожна зміна слова або тегу впливає на аналіз. Своєю чергою, зміни в аналізаторі проходять регресійні тести на 100-мільйонному архіві українських текстів, зібраних із друкованих медій, української літератури та інших джерел.
Отже, що словник, що аналізатор LT взоруються на академічні матеріали та підходи, однак мають виразне практичне спрямування.

## Розбиття на речення

Аналіз тексту починається з розбиття на речення. На перший погляд це досить просте завдання — знайти крапку (або знаки оклику і питання) і позначити межу речення. Насправді ж такий простий підхід дає досить велику кількість помилок через те, що крапки часто використовують у скороченнях, ініціалах, датах тощо. Ця проблема властива більшості європейських мов, йцілком задовільно її розв’язати неможливо, бо в багатьох таких випадках розрізнити вживання крапки можна лише на семантичному рівні. А проте якість розбиття можна довести до достатньо якісного рівня, якщо проаналізувати й використати логіку вживання крапки для скорочень та ініціалів. Український модуль у LanguageTool має близько 30 правил для таких випадків, щоб запобігти зайвим розбиттям.

## Розбиття на слова

Після того, як ми отримали речення їх потрібно розбити на слова (лексеми). Тут теж є свої тонкощі: скажімо, кома може бути розділовим знаком або частиною десяткового дробу. Групи тисяч у числах часто розділяють пробілами. Дати та час записують через крапку й двокрапку. Ініціали часто пишуть разом із прізвищем, а дужки можуть бути частиною скорочення, наприклад,ОУН(б). Також в реальних текстах вживають кілька різних символів на позначення апострофа.
Модуль українського лексемування (токенізації) в LanguageTool містить близько 20 регулярних виразів для виловлювання таких складних випадків.

## Тегування

Якщо під рукою є добрий словник тегів (як-от ВЕСУМ) і код застосування (як-от morfologik), завдання тегування слів у своїй основі є досить тривіальним. Однак як і на попередніх стадіях розбиття, тут є свої моменти. Одна з найбільших проблем — це складні слова. Хай який великий не є словник, у нього годі вмістити всі можливі слова, що пишуться через дефіс. Кількість таких комбінацій величезна, що зробить словник непомірно великим і незастосовним. А наявність у текстах цифрово-абеткових прикметників, наприклад,133-тя, показує, що тегування, базоване винятково на словнику, має серйозні обмеження.
Отже, потрібна логіка тегування таких складних слів, яких немає в словнику, але які утворюються зі слів, яким уже присвоєно теги. Наприклад, якщо протегувати обидві частини у слові ”ракет-носіїв” то можна спробувати протегувати й ціле складне слово. Звісно, тут, як і в більшості проблем алгоритмізації природної мови, є маса нюансів.Скажімо, що робити, якщо ліва частина позначає істоту, а права — ні? Чи належатиме таке складне слово до категорії істот? Виявляється, що в багатьох випадках, проаналізувавши наявні тексти й побудувавши допоміжні словники, можна з високою ймовірністю правильно розв’язати цю задачу.
Як показав досвід (після тривалих експериментів та набитих ґуль), модуль динамічного тегування LanguageTool може тегувати десятки тисяч таких не охоплених словниками слів із достатньою для практичного використання точністю.

## Зняття омонімії

Одним із найскладніших моментів алгоритмізації оброблювання природної мови є наявність омонімії. Більшість слів, протегованих за словником, матимуть більше одного набору тегів.Скажімо, слово “формасиній” може відповідати називному, знахідному або кличному відмінку. Велика частина таких омонімічних випадків матиме більше ніж одну лему. Наприклад, слово “біль” може бути як ч.р. (відчуття), так і ж.р. (білі нитки), також трапляється міжчастиномовна омонімія: “за” може вживатися як прийменник або як прислівник.
Можна створити деякі правила, що допоможуть автоматично зняти омонімію лише для невеликої частини слів, навіть часткове зняття може значно підвищити якість аналізу.
Наразі LanguageTool має лише декілька десятків правил автоматичного зняття омонімії. Вдосконалення цього підмодуля — один із пріоритетних напрямків розвитку.

## Застосування аналізу для перевірки граматики

Спроможність тегувати слова уможливлює перевірку на граматичну коректність і виловлювання хибних конструкцій. Хоча граматичну, пунктуаційну та стилістичну перевірку можна провадити й без словника, наприклад, виловлюючи конструкції регулярними виразами (є ПЗ для української мови, що ґрунтуються на цьому підході), оперування лемами та інформацією про відмінювання піднімає аналіз на якісно інший рівень.	

## Синтезатор словоформ

Ще одне цікаве застосування тегової інформації — отримання словоформ слова за заданим відмінком, часом тощо. Ця можливість широко використовується в LanguageTool для генерування правильних словоформ замість помилкових. 
## ЛІТЕРАТУРА
1. Обробка природної мови - Режим доступу: https://ru.wikipedia.org/wiki/Обработка_естественного_языка
2. Neural Machine Translation (seq2seq) Tutorial - Режим доступу: https://www.tensorflow.org/tutorials/seq2seq
3. Word2vec - Режим доступу: https://ru.wikipedia.org/wiki/Word2vec
4. LSTM - мережі довгої короткостроковій пам'яті - Режим доступу: https://habrahabr.ru/company/wunderfund/blog/331310/
5. NLP - Режим доступу: https://habr.com/ru/company/abbyy/blog/437008/
6. Огляд досліджень в області глибокого навчання: обробка природних мов - Режим доступу: https://habr.com/ru/company/wunderfund/blog/330194/
7. Режим доступу: http://ena.lp.edu.ua:8080/bitstream/ntb/29776/1/21_197-202.pdf
8. Режим доступу: http://wikiinfo.mdpu.org.ua/index.php?title=Синтаксичний_аналіз
9. Режим доступу: https://r2u.org.ua/articles/vesum
10. Режим доступу: https://github.com/brown-uk/dict_uk/blob/master/doc/announcement.md
11. Режим доступу: https://dou.ua/lenta/articles/dou-projector-dict-uk/
